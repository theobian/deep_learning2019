{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Module\n",
    "Forward and Backward passes need to be defined explicitly\n",
    "For Activation and Loss Functions, the backward pass is defined by the forward's gradient\n",
    "'''\n",
    "import torch\n",
    "from torch import FloatTensor\n",
    "from torch import LongTensor\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    '''\n",
    "    input: self, string identifier\n",
    "    output: none\n",
    "    set id to None by default\n",
    "    standard instantiation\n",
    "    '''\n",
    "    def __init__(self, id):\n",
    "        if(id == None):\n",
    "            self.id = 'None'\n",
    "        else :\n",
    "            self.id = id\n",
    "\n",
    "\n",
    "    '''\n",
    "    has to be overridden\n",
    "    '''\n",
    "    def forward(self, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    '''\n",
    "    has to be overridden\n",
    "    '''\n",
    "    def backward(self, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    '''\n",
    "    has to be overridden\n",
    "    '''\n",
    "    def param(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    '''\n",
    "    does not have to be overridden\n",
    "    should be overridden for Module instances that have parameter gradients\n",
    "    '''\n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    '''\n",
    "    input: input and output sizes for layer determination, weights and biases and standard deviation\n",
    "    output: none\n",
    "    weights and biases are by default initialized with the std provided according to a normal distribution\n",
    "    gradients for weights and biases are set to zero\n",
    "    standard instantiation\n",
    "    '''\n",
    "    def __init__(self, input, output, std, w = None, b = None):\n",
    "        Module.__init__(self, 'Linear')\n",
    "        if(w != None):\n",
    "            self.w = FloatTensor(output,input).normal_(0, std)\n",
    "        self.w = FloatTensor(output,input).normal_(0, std)\n",
    "        if(b != None):\n",
    "            self.b = FloatTensor(output).normal_(0, std)\n",
    "        self.b = FloatTensor(output).normal_(0, std)\n",
    "        self.dw = FloatTensor(output,input).zero_()\n",
    "        self.db = FloatTensor(output).zero_()\n",
    "\n",
    "    \n",
    "\n",
    "    '''\n",
    "    sets gradients to zero\n",
    "    '''    \n",
    "    def zero_grad(self):\n",
    "        self.dw.zero_()\n",
    "        self.db.zero_()\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    input: self, input\n",
    "    output: input after forward pass\n",
    "    this computes forward according to the formula: input * weight + bias\n",
    "    the input is flattened into a vector and weights are multipled with it\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        s = self.w.mv(x.view(-1)) + self.b\n",
    "        return s\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    input: input, gradient\n",
    "    output: gradient from before forward pass\n",
    "    this allows the backpropagation of errors\n",
    "    the weight and biases are updated thanks to the loss function gradient parameter\n",
    "    '''\n",
    "    def backward(self, x, dl_dx):\n",
    "        self.dw.add_(dl_dx.view(-1, 1).mm(x.view(1, -1)))\n",
    "        self.db.add_(dl_dx.view(-1))\n",
    "        dx_previous = self.w.t().mm(dl_dx)\n",
    "        return dx_previous\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    returns a list of parameters to use for optimization\n",
    "    '''\n",
    "    def param(self):\n",
    "        return [[self.w, self.dw], [self.b, self.db]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    '''\n",
    "    input: self\n",
    "    output: none\n",
    "    standard instantiation\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        Module.__init__(self, 'Relu')\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    input: self, input\n",
    "    output: input after forward pass\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    input: self, input, gradient\n",
    "    output: backward pass propagation according to input\n",
    "    allows the computation of error backward propagation\n",
    "    '''\n",
    "    def backward(self, x, dl_dx):\n",
    "        mask = (x > 0).float()\n",
    "        x = torch.mul(x, mask).view(-1, 1)\n",
    "        return torch.mul(x, dl_dx)\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    input: self\n",
    "    output: None \n",
    "    differentiates this Module instance from those with parameters\n",
    "    '''\n",
    "    def param(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    '''\n",
    "    input: self\n",
    "    output: none\n",
    "    standard instantiation\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        Module.__init__(self, 'Tanh')\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    input: self, input\n",
    "    output: input after forward pass\n",
    "    '''\n",
    "    def forward(self,s0):\n",
    "        return s0.tanh()\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    input: self, input, gradient\n",
    "    output: backward pass propagation according to input\n",
    "    allows the computation of error backward propagation\n",
    "    '''\n",
    "    def backward(self, x, dl_dx):\n",
    "        return 4 * (x.view(-1,1).exp() + x.view(-1,1).mul(-1).exp()).pow(-2) * dl_dx\n",
    "    \n",
    "\n",
    "\n",
    "    '''\n",
    "    input: self\n",
    "    output: None \n",
    "    differentiates this Module instance from those with parameters\n",
    "    '''\n",
    "    def param(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    '''\n",
    "    input: self\n",
    "    output: none\n",
    "    standard instantiation\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        Module.__init__(self, 'MSE')\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    input: self, input\n",
    "    output: input after forward pass\n",
    "    '''\n",
    "    def forward(self, v, t):\n",
    "        return (v.view(2,1) - t).pow(2).sum()\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    input: self, input, gradient\n",
    "    output: backward pass propagation according to input\n",
    "    allows the computation of error backward propagation\n",
    "    '''\n",
    "    def backward(self, v, t):\n",
    "        return 2 * (v.view(2,1) - t)\n",
    "    \n",
    "\n",
    "\n",
    "    '''\n",
    "    input: self\n",
    "    output: None \n",
    "    differentiates this Module instance from those with parameters\n",
    "    '''\n",
    "    def param(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sequential\n",
    "'''\n",
    "import torch\n",
    "from torch import FloatTensor\n",
    "from torch import LongTensor\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential():\n",
    "    '''\n",
    "    input: self\n",
    "    output: none\n",
    "    within elements, each element is an instance of the Module class\n",
    "    the cache contains an array of an input after it is passed through each forward pass\n",
    "    standard instantiation\n",
    "    '''\n",
    "    def __init__(self, *args):\n",
    "        self.elements = args\n",
    "        self.cache = []\n",
    "\n",
    "\n",
    "     \n",
    "    '''\n",
    "    input: self, data\n",
    "    output: input after having gone through all Module instances' forward pass\n",
    "    the forward pass computes an output through a list of Module instances\n",
    "    the starting input, as well as each input after it is passed through a layer, is saved\n",
    "    this list of saved inputs is necessary to compute the backward-propagatino gradients\n",
    "    '''    \n",
    "    def forward(self, input):\n",
    "        self.cache = []\n",
    "        self.cache.append(input)\n",
    "        for i, elt in enumerate(self.elements):\n",
    "            input = elt.forward(input);\n",
    "            self.cache.append(input)\n",
    "        return input\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    input: self, data and model gradient -- weights and bias gradients\n",
    "    output: gradient\n",
    "    the backward pass computes an output through a list of Module instances\n",
    "    the starting gradient as well as each gradient after it is passed through a layer, is saved\n",
    "    the backpropagation can happen because a list of inputs at each layer/point has been cached\n",
    "    this backpropagation algorithm in fine allows the update of weights\n",
    "    '''\n",
    "    def backward(self, input, grad):\n",
    "        error = []\n",
    "        error.append(grad)\n",
    "        for i, elt in reversed(list(enumerate(self.elements))):\n",
    "            grad = elt.backward(self.cache[i], grad)\n",
    "            error.append(grad)\n",
    "        error.reverse()\n",
    "        return grad, error\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    input: self\n",
    "    output: none\n",
    "    iterates over elements within the Sequential instance and sets their respective gradients to zero\n",
    "    this is used after each model training iteration, to make sure all gradients are computed over one iteration\n",
    "    '''\n",
    "    def zero_grad(self):\n",
    "        for element in self.elements:\n",
    "            element.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    input: self\n",
    "    output: parameters of the whole sequence as a list\n",
    "    iterates over elements within the Sequential instance, and fetches their parameters\n",
    "    this is used to get parameters (weights, biases, and their gradients) for optimization\n",
    "    '''\n",
    "    def param(self):\n",
    "        parameters = []\n",
    "        for elt in self.elements:\n",
    "            if elt.param() != None:\n",
    "                parameters += elt.param()\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Optimizer\n",
    "'''\n",
    "import torch\n",
    "from torch import FloatTensor\n",
    "from torch import LongTensor\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization():\n",
    "    '''\n",
    "    needs to be overridden\n",
    "    '''\n",
    "    def __init__(self, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimization):\n",
    "    '''\n",
    "    input: self, parameters to be optimized, learning rate\n",
    "    output: none\n",
    "    standard instantiation\n",
    "    '''\n",
    "    def __init__(self, parameters, eta):\n",
    "        self.eta = eta\n",
    "        self.parameters = parameters\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    input: self\n",
    "    output: none\n",
    "    Iterates over the parameters and updates them according to the following rule:\n",
    "    param = gradient_param * learning_rate\n",
    "    '''\n",
    "    def step(self):\n",
    "        for w in self.parameters:\n",
    "            w[0]-= self.eta * w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum(Optimization):\n",
    "    '''\n",
    "    input: self, parameters to be optimized, learning rate, update rate/momentum\n",
    "    output: none\n",
    "    standard instantiation\n",
    "    '''\n",
    "    def __init__(self, parameters, eta, gamma = 0.9):\n",
    "        self.parameters = input_parameters\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    input: self, previous update vector\n",
    "    output: none\n",
    "    iterates over the parameters and updates them according to the following rule:\n",
    "    param = gradient_param * learning_rate\n",
    "    updates the learning rate according to the following rate:\n",
    "    learning_rate = momentum * previous_update_vector\n",
    "    '''\n",
    "    def step(self, prev_update):\n",
    "        for w in self.parameters:\n",
    "            w[0] -= self.eta * w[1]\n",
    "            self.eta += (self.gamma * prev_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utils\n",
    "'''\n",
    "import math\n",
    "import torch\n",
    "from torch import FloatTensor\n",
    "from torch import LongTensor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input: number of data points to generate\n",
    "output: binary input and corresponding class according to the following rule: \n",
    "target is 1 if point pair is within circle of radius R = sqrt(1/2pi)\n",
    "''' \n",
    "def data_gen(n):\n",
    "    input = FloatTensor(n, 2).uniform_(0, 1)\n",
    "    target = FloatTensor(n, 2).uniform_(0, 1)\n",
    "    target = input.pow(2).sum(1).sub(1 / (2*math.pi)).sign().add(1).div(2).float()\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input: input and class to be plotted\n",
    "output: none\n",
    "plots the binary data as two different classes according to their labeled class\n",
    "saves the figure\n",
    "'''\n",
    "def data_plot(input, target):\n",
    "    cmap = []\n",
    "    for i in range(len(target)):\n",
    "        if(target[i]):\n",
    "            cmap.append('green')\n",
    "        else:\n",
    "            cmap.append('red')\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.scatter(input[:,0], input[:,1], s=1,c = cmap)\n",
    "    circle1 = plt.Circle((0,0),np.sqrt(1/(2*math.pi)), color = 'black', fill = False)\n",
    "    plt.gcf().gca().add_artist(circle1)\n",
    "    plt.ylim(0, 1);\n",
    "    plt.xlim(0, 1);\n",
    "    plt.title('Dataset')\n",
    "    plt.show()\n",
    "    fig.savefig('Dataset.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input: model to be trained; binary data and corresponding class; loss and optimization criterions for training, learning rate, epochs, and verbose parameter\n",
    "output: total losses for each epoch, and model parameters (weights, biases and corresponding gradients) after the last epoch training\n",
    "this is SGD type update;\n",
    "iteratively calls: the model's forward pass, then the loss's foward pass for a single binary data point and class\n",
    "then adds the running loss to the total loss\n",
    "then set all gradients to zero before computing the model's backward pass for a single binary data point and the loss function's gradient\n",
    "then updates the parameter weights through the optimizer.\n",
    "'''\n",
    "def train(model, train_input, train_target, loss_criterion, optimizer, eta, epochs, verbose):\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        for s in range(len(train_input)):\n",
    "            output = model.forward(train_input.narrow(0, s, 1))\n",
    "            loss = loss_criterion.forward(output, train_target.narrow(0, s, 1))\n",
    "            sum_loss += loss.item()\n",
    "            model.zero_grad()\n",
    "            grad, grad_err = model.backward(output, loss_criterion.backward(output, train_target.narrow(0, s, 1)))\n",
    "            optimizer.step()\n",
    "        losses.append(sum_loss)\n",
    "        if((e%5 == 0 or e == 0 or e == epochs) and verbose):\n",
    "            print('epoch', e,'loss', sum_loss)\n",
    "    weights = model.param()\n",
    "    return losses, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input: model to be trained; binary data and corresponding class; loss criterion for testing, and verbose parameter\n",
    "output: number of misclassified points, coordinates for the vector of misclassified as well as for classified points, and a label vector of the same size as the input data\n",
    "for each sample within the test set: computes the model forward to precict label as a probability of being one class or the other\n",
    "the most likely prediction (index corresponding to the max value of the last layer's output) is compared to the actual label \n",
    "errors, points, and labels that were misclassifed are logged accordingly\n",
    "the l parameter returned makes it easier to keep track of which labels were misclassified\n",
    "'''\n",
    "def eval(model, test_input, test_target, loss_criterion, verbose):\n",
    "    n_errors = 0\n",
    "    cx, cy, ix, iy = [], [], [], []\n",
    "    l = []\n",
    "    for s in range(len(test_input)):\n",
    "        output = model.forward(test_input.narrow(0, s, 1))\n",
    "        t = test_target.narrow(0, s, 1).item()\n",
    "        i = test_input.narrow(0, s, 1)\n",
    "        p = np.argmax(output)\n",
    "        if(t == p):\n",
    "            cx.append(i[0][0].item())\n",
    "            cy.append(i[0][1].item())\n",
    "            l.append(1)\n",
    "        else:\n",
    "            n_errors += 1\n",
    "            ix.append(i[0][0].item())\n",
    "            iy.append(i[0][1].item())\n",
    "            l.append(0)\n",
    "    return n_errors, ix, iy, cx, cy, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input: incorrectly labeled and correctl labeled point coordinates (for binary data)\n",
    "output: none\n",
    "plots the data points, with two classes -- not according to the true label (which can be determined with the circle boundary)),\n",
    "but according to the classification truth\n",
    "'''\n",
    "def plot_results(ix, iy, cx, cy):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.scatter(ix, iy, s = 1, c = 'red', label = 'Misclassified')\n",
    "    plt.scatter(cx, cy, s = 1, c = 'green', label = 'Correctly Classified')\n",
    "    boundary = plt.Circle((0,0),np.sqrt(1/(2*math.pi)), color = 'black', fill = False)\n",
    "    plt.gcf().gca().add_artist(boundary)\n",
    "    plt.ylim(-0.01, 1.1);\n",
    "    plt.xlim(-0.01, 1.1);\n",
    "    plt.legend(loc = 1)\n",
    "    plt.title('Results')\n",
    "    plt.show()\n",
    "    fig.savefig('Results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input: text to be outputted, file idenifier for easy saving\n",
    "output: none\n",
    "allows saving model results to csv format file\n",
    "'''\n",
    "def write_to_csv(text, file_id):\n",
    "\n",
    "    with open('Output/test_{}.csv'.format(file_id), mode = 'w') as to_csv:\n",
    "        for i in range(len(text)):\n",
    "            to_csv.write(text[i])\n",
    "            to_csv.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# test\n",
    "# '''\n",
    "# from Sequential import *\n",
    "# from Module import *\n",
    "# from Optimizer import *\n",
    "# from Utils import *\n",
    "# import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "main\n",
    "creates a dataset, a model, an optimizer, a loss criterion, with learning rate, level of layer standard deviation initialization for weights and biases, batch size\n",
    "prints losses and training/testing errors\n",
    "'''\n",
    "def main():\n",
    "\n",
    "\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    verbose = False\n",
    "    n_train, n_test = 1000, 1000\n",
    "    train_input, train_target = data_gen(n_train)\n",
    "    test_input, test_target = data_gen(n_test)\n",
    "    std = 1e-1\n",
    "    batch_size = 20\n",
    "    epochs = 50\n",
    "    eta = 1e-1\n",
    "\n",
    "\n",
    "    model = Sequential(Linear(2,25, std),\n",
    "                        Tanh(),\n",
    "                        Linear(25,25, std),\n",
    "                        Tanh(),\n",
    "                        Linear(25,25, std),\n",
    "                        Tanh(),\n",
    "                        Linear(25,25, std),\n",
    "                        Tanh(),\n",
    "                        Linear(25,2, std))\n",
    "\n",
    "\n",
    "    optimizer = SGD(model.param(), eta)\n",
    "\n",
    "\n",
    "    loss_criterion = MSELoss()\n",
    "\n",
    "\n",
    "    print('\\n Training...')\n",
    "    losses, weights = train(model, train_input, train_target, loss_criterion, optimizer, eta, epochs, verbose)\n",
    "    print('\\n Testing...')\n",
    "    n_error_train, ix_train, iy_train, cx_train, cy_train, l_train = eval(model, train_input, train_target, loss_criterion, verbose)\n",
    "    n_error_test,  ix_test, iy_test, cx_test, cy_test,l_test = eval(model, test_input, test_target, loss_criterion, verbose)\n",
    "    print('train error {} %'.format(n_error_train/len(train_input)))\n",
    "    print('test error {} %'.format(n_error_test/len(test_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training...\n",
      "\n",
      " Testing...\n",
      "train error 0.582 %\n",
      "test error 0.576 %\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen_visualization(n):\n",
    "    print('n =', n, '\\n \\n')\n",
    "    input = FloatTensor(n, 2).uniform_(0, 1)\n",
    "    print('input = ', input, '\\n', 'input size =', input.size(), '\\n \\n')\n",
    "    target = FloatTensor(n, 2).uniform_(0, 1)\n",
    "    print('target1 = ', target, '\\n', 'target1 size =', target.size(), '\\n \\n')\n",
    "    target = input.pow(2).sum(1).sub(1 / (2*math.pi)).sign().add(1).div(2).float()\n",
    "    print('target2 = ', target, '\\n', 'target2 size =', target.size(), '\\n \\n')\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 5 \n",
      " \n",
      "\n",
      "input =  tensor([[0.0973, 0.1485],\n",
      "        [0.6530, 0.9692],\n",
      "        [0.3458, 0.3742],\n",
      "        [0.0394, 0.4236],\n",
      "        [0.1115, 0.7990]]) \n",
      " input size = torch.Size([5, 2]) \n",
      " \n",
      "\n",
      "target1 =  tensor([[0.9688, 0.8497],\n",
      "        [0.4597, 0.1884],\n",
      "        [0.5708, 0.6112],\n",
      "        [0.8108, 0.5679],\n",
      "        [0.7561, 0.3849]]) \n",
      " target1 size = torch.Size([5, 2]) \n",
      " \n",
      "\n",
      "target2 =  tensor([0., 1., 1., 1., 1.]) \n",
      " target2 size = torch.Size([5]) \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_, target_ = data_gen_visualization(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
