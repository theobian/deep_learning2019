MUST INCLUDE:
!! weight sharing !!
!! auxiliary losses !!
10 + rounds per model architecture with: data and weight initialization randomized and providing standard deviation estimates

PARAMETERS:
layer number: also on a small range
learning rate: play around just a bit on a small range
optimizer: ADAM vs stochastic gradient procedure to update weights, etc... ADAM should be the best
activation functions: ReLU, leaky ReLU, CeLU (Continuously Differentiable Exponential Linear Units) should be the best
pooling: max pooling vs average pooling vs sum pooling
dropout: yes/no, random vs based on value
weight sharing: yes/no/batch size

MODELS:
- MLP: pure
- CNN: pure
- CNN: 'fancy' mix
- CNN: state-of-the-art

SUBMISSION:
- 1000 pairs for test and training each
- convnet with 70k parameters can be trained with 25 epochs shuld train in less than 2s and achieve ~15% error

IDEAS:
- normalize the input data: 0-255 --> 0-1
- or should we do 0-255 --> -1-1
- keep track of weights + errors per model
- set random seed
- should probably have a reduced range of training epochs or use the same number everytime.
